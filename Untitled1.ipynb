{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e686df2d-a60b-4b13-bd1c-ae7b49c5f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from enum import Enum\n",
    "\n",
    "# grid_size = 5\n",
    "\n",
    "# class Action(Enum):\n",
    "#     UP = 0\n",
    "#     DOWN = 1\n",
    "#     LEFT = 2\n",
    "#     RIGHT = 3\n",
    "\n",
    "# class QTable:\n",
    "#     def __init__(self, row, col, num_actions):\n",
    "#         self.values = np.zeros((row, col, num_actions))\n",
    "\n",
    "#     def get_best_action(self, row, col):\n",
    "#         return np.argmax(self.q_table[row][col])\n",
    "        \n",
    "\n",
    "# class SarsaAgent:\n",
    "#     def __init__(self, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        \n",
    "#         # NFT hyperperameters\n",
    "#         self.alpha = alpha  # Learning rate\n",
    "#         self.gamma = gamma  # Discount factor\n",
    "#         self.epsilon = epsilon  # Exploration rate\n",
    "        \n",
    "#         # Learning\n",
    "#         self.last_state = None\n",
    "#         self.last_action = None\n",
    "\n",
    "#     # Engine\n",
    "#     def set_q_table(q_table):\n",
    "#         self.q_table = q_table\n",
    "\n",
    "#     def choose_action(self, state):\n",
    "#         if np.random.uniform(0, 1) < self.epsilon:\n",
    "#             return np.random.randint(self.num_actions)\n",
    "#         else:\n",
    "#             return self.q_table.get_best_action(state)\n",
    "\n",
    "#     def learn(self, state, action, reward, next_state, next_action):\n",
    "#         if self.last_state is not None:\n",
    "#             # Update Q-value using SARSA\n",
    "#             self.q_table.q_table[self.last_state, self.last_action] += self.alpha * (\n",
    "#                     reward + self.gamma * self.q_table.q_table[next_state, next_action] -\n",
    "#                     self.q_table.q_table[self.last_state, self.last_action])\n",
    "#         self.last_state = next_state\n",
    "#         self.last_action = next_action\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.last_state = None\n",
    "#         self.last_action = None\n",
    "\n",
    "\n",
    "# class GridWorld:\n",
    "#     def __init__(self, size=5):\n",
    "#         self.size = size\n",
    "#         self.reward = np.zeros((size, size))\n",
    "\n",
    "#         # Define rewards for each cell\n",
    "#         self.goal = (3, 3)\n",
    "#         self.reward[3, 3] = 1  # Reward of +1 at cell (3, 3)\n",
    "#         self.state = None\n",
    "#         self.reset()\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.state = self.get_state_hash(0, 0)\n",
    "#         return self.state\n",
    "\n",
    "#     def get_state_hash(self, row, col):\n",
    "#         return (row * self.size) + col\n",
    "\n",
    "#     def step(self, action):\n",
    "#         row, col = divmod(self.state, self.size)\n",
    "\n",
    "#         if action == Action.RIGHT.value:  # Move right\n",
    "#             col = min(col + 1, self.size - 1)\n",
    "#         elif action == Action.LEFT.value:  # Move left\n",
    "#             col = max(col - 1, 0)\n",
    "#         elif action == Action.DOWN.value:  # Move down\n",
    "#             row = min(row + 1, self.size - 1)\n",
    "#         elif action == Action.UP.value:  # Move up\n",
    "#             row = max(row - 1, 0)\n",
    "\n",
    "#         self.state = self.get_state_hash(row, col)\n",
    "#         reward_value = self.reward[row, col]\n",
    "#         done = self.state == self.get_state_hash(self.goal[0], self.goal[1])  # Terminate if reached the goal\n",
    "#         return self.state, reward_value, done, {}\n",
    "\n",
    "#     def render(self):\n",
    "#         for i in range(self.size):\n",
    "#             for j in range(self.size):\n",
    "#                 if self.get_state_hash(i, j) == self.state:\n",
    "#                     print(\"x\", end=\" \")\n",
    "#                 elif (i, j) == self.goal:\n",
    "#                     print(\"G\", end=\" \")\n",
    "#                 else:\n",
    "#                     print(\"-\", end=\" \")\n",
    "#             print()\n",
    "#         print()\n",
    "#         print()\n",
    "\n",
    "\n",
    "# def train_agent(env, agent, num_episodes=100, render=False):\n",
    "#     for episode in range(1, num_episodes + 1):\n",
    "#         state = env.reset()\n",
    "#         action = agent.choose_action(state)\n",
    "#         agent.reset()\n",
    "#         done = False\n",
    "\n",
    "#         while not done:\n",
    "#             if render:\n",
    "#                 env.render()\n",
    "#             next_state, reward, done, _ = env.step(action)\n",
    "#             next_action = agent.choose_action(next_state)\n",
    "#             agent.learn(state, action, reward, next_state, next_action)\n",
    "#             state = next_state\n",
    "#             action = next_action\n",
    "#     if render:\n",
    "#         env.render()\n",
    "\n",
    "# # Define environment and agent\n",
    "# print(\"Starting Gridworld\")\n",
    "# env = GridWorld()\n",
    "# env.render()\n",
    "\n",
    "# ns = grid_size * grid_size\n",
    "# na = len(list(Action))\n",
    "# print(ns, na)\n",
    "\n",
    "# agent = SarsaAgent(num_states=ns, num_actions=na)  # Assuming 10x10 grid world\n",
    "# agent.set_q_table(QTable(ns, na))\n",
    "\n",
    "\n",
    "# # Train the agent\n",
    "# train_agent(env, agent, 100)\n",
    "# print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccbebf87-2b9d-49c1-a80a-1ff4189e019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fdbc990-de5c-4351-8484-0eb8b02ae727",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10280f95-80d2-4ddf-8814-6fce6561ae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Action(Enum):\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "468e5b06-a3ea-4734-a3cc-b81b0866db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.reward = np.zeros((size, size))\n",
    "\n",
    "        # Define rewards for each cell\n",
    "        self.goal = (3, 3)        \n",
    "        self.state = None\n",
    "        self.reset()\n",
    "\n",
    "    def get_reward(self, state):\n",
    "        if state == self.goal:\n",
    "            return 1\n",
    "        return 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        row, col = self.state\n",
    "\n",
    "        if action == Action.RIGHT.value:  # Move right\n",
    "            col = min(col + 1, self.size - 1)\n",
    "        elif action == Action.LEFT.value:  # Move left\n",
    "            col = max(col - 1, 0)\n",
    "        elif action == Action.DOWN.value:  # Move down\n",
    "            row = min(row + 1, self.size - 1)\n",
    "        elif action == Action.UP.value:  # Move up\n",
    "            row = max(row - 1, 0)\n",
    "\n",
    "        self.state = (row, col)\n",
    "        reward_value = self.get_reward(self.state)\n",
    "        done = self.state == self.goal  # Terminate if reached the goal\n",
    "        return self.state, reward_value, done, {}\n",
    "\n",
    "    def render(self):\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if (i, j) == self.state:\n",
    "                    print(\"x\", end=\" \")\n",
    "                elif (i, j) == self.goal:\n",
    "                    print(\"G\", end=\" \")\n",
    "                else:\n",
    "                    print(\"-\", end=\" \")\n",
    "            print()\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02d5edcb-a4d0-40fd-a408-f761295f8d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    def __init__(self, row, col, num_actions):\n",
    "        self.values = np.zeros((row, col, num_actions))\n",
    "\n",
    "    def get_best_action(self, row, col):\n",
    "        return np.argmax(self.values[row][col])\n",
    "\n",
    "    def get_value(self, row, col, action):\n",
    "        return self.values[row][col][action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44e6d91f-3f29-410f-8ca0-55d0b9beca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent:\n",
    "    def __init__(self, num_actions, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # NFT hyperperameters\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        \n",
    "        # Learning\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "\n",
    "    # Engine\n",
    "    def set_q_table(self, q_table):\n",
    "        self.q_table = q_table\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            row, col = state\n",
    "            return self.q_table.get_best_action(row, col)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, next_action):\n",
    "        next_row, next_col = next_state        \n",
    "        if self.last_state is not None:\n",
    "            # Update Q-value using SARSA\n",
    "            current_q = self.q_table.get_value(next_row, next_col, next_action)\n",
    "            last_q = self.q_table.values[self.last_state, self.last_action]\n",
    "            update = self.alpha * (reward + self.gamma * current_q - last_q)\n",
    "            self.q_table.values[self.last_state, self.last_action] += update\n",
    "        self.last_state = next_state\n",
    "        self.last_action = next_action\n",
    "\n",
    "    def reset(self):\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ecfd6b-681d-40dc-a427-7777c4b94448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff47bc3c-8fdb-40ae-b80c-4c2e2f3d3f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent, num_episodes=100, render=False):\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        action = agent.choose_action(state)\n",
    "        agent.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_action = agent.choose_action(next_state)\n",
    "            agent.learn(state, action, reward, next_state, next_action)\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "    if render:\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "270ae400-ef61-4914-b847-1d7cac6f89e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Gridworld\n",
      "x - - - - \n",
      "- - - - - \n",
      "- - - - - \n",
      "- - - G - \n",
      "- - - - - \n",
      "\n",
      "\n",
      "25 4\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "# Define environment and agent\n",
    "print(\"Starting Gridworld\")\n",
    "env = GridWorld(grid_size)\n",
    "env.render()\n",
    "\n",
    "ns = grid_size * grid_size\n",
    "na = len(list(Action))\n",
    "print(ns, na)\n",
    "\n",
    "q = QTable(grid_size, grid_size, na)\n",
    "\n",
    "agent = SarsaAgent(na)\n",
    "agent.set_q_table(q)\n",
    "\n",
    "\n",
    "# Train the agent\n",
    "train_agent(env, agent, 1)\n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26571657-3759-4abb-aca7-5bb517b843ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0. ],\n",
       "        [0.1, 0.1, 0.1, 0.1],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0. ],\n",
       "        [0.1, 0.1, 0.1, 0.1],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51709ca-dba5-4ad2-beca-fedc30d23aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
