{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "size = 10\n",
    "class GridWorld:\n",
    "    def __init__(self, size=10):\n",
    "        self.size = size\n",
    "        self.grid = np.zeros((size, size))\n",
    "        # Define rewards for each cell\n",
    "        self.grid[3, 3] = 1  # Reward of +1 at cell (3, 3)\n",
    "        self.grid[7, 7] = -1  # Reward of -1 at cell (7, 7)\n",
    "        self.state = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:  # Move right\n",
    "            self.state = (self.state[0], min(self.state[1] + 1, self.size - 1))\n",
    "        elif action == 1:  # Move left\n",
    "            self.state = (self.state[0], max(self.state[1] - 1, 0))\n",
    "        elif action == 2:  # Move down\n",
    "            self.state = (min(self.state[0] + 1, self.size - 1), self.state[1])\n",
    "        elif action == 3:  # Move up\n",
    "            self.state = (max(self.state[0] - 1, 0), self.state[1])\n",
    "\n",
    "        reward = self.grid[self.state[0], self.state[1]]\n",
    "        done = (self.state == (self.size - 1, self.size - 1))  # Terminate if reached bottom-right corner\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def render(self):\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if (i, j) == self.state:\n",
    "                    print(\"x\", end=\" \")\n",
    "                else:\n",
    "                    print(\"-\", end=\" \")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: (0, 0)\n",
      "GridWorld:\n",
      "x - - - - - - - - - \n",
      "- - - - - - - - - - \n",
      "- - - - - - - - - - \n",
      "- - - - - - - - - - \n",
      "- - - - - - - - - - \n",
      "- - - - - - - - - - \n",
      "- - - - - - - - - - \n",
      "- - - - - - - - - - \n",
      "- - - - - - - - - - \n",
      "- - - - - - - - - - \n",
      "Taking action: 0\n",
      "Next State: (0, 1)\n",
      "Reward: 0.0\n",
      "Done: False\n",
      "GridWorld:\n",
      "- x - - - - - - - - \n",
      "- - - - - - - - - - \n",
      "- - - - - - - - - - \n",
      "- - - - - - - - - - \n",
      "- - - - - - - - - - \n",
      "- - - - - - - - - - \n",
      "- - - - - - - - - - \n",
      "- - - - - - - - - - \n",
      "- - - - - - - - - - \n",
      "- - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "env = GridWorld(size)\n",
    "print(\"Initial State:\", env.reset())\n",
    "print(\"GridWorld:\")\n",
    "env.render()\n",
    "action = 0  # Move right\n",
    "print(\"Taking action:\", action)\n",
    "next_state, reward, done, _ = env.step(action)\n",
    "print(\"Next State:\", next_state)\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Done:\", done)\n",
    "print(\"GridWorld:\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen action: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Holds a preference for which action to perform in which state\n",
    "class QTable:\n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.q_table = np.zeros((num_states, num_actions, 4))  # 4 actions: right, left, down, up\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        return np.argmax(self.q_table[state[0], state[1]])\n",
    "\n",
    "# Interacts with the environment\n",
    "class SarsaAgent:\n",
    "    def __init__(self, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        # These defined by NFT for rarity\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "\n",
    "        # Sets the learning\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.uniform(0, 1) < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            return self.q_table.get_best_action(state)\n",
    "\n",
    "    def learn(self, state, action, reward, next_state, next_action):\n",
    "        if self.last_state is not None:\n",
    "            # Update Q-value using SARSA\n",
    "            self.q_table.q_table[self.last_state, self.last_action] += self.alpha * (\n",
    "                    reward + self.gamma * self.q_table.q_table[next_state, next_action] -\n",
    "                    self.q_table.q_table[self.last_state, self.last_action])\n",
    "        self.last_state = next_state\n",
    "        self.last_action = next_action\n",
    "\n",
    "    def reset(self):\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "\n",
    "    def add_q_table(self, q_table):\n",
    "        self.q_table = q_table\n",
    "\n",
    "# Example usage:\n",
    "agent = SarsaAgent()\n",
    "q_table = QTable(size * size, )\n",
    "state = (0, 0)\n",
    "action = agent.choose_action(state)\n",
    "print(\"Chosen action:\", action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 1 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m agent \u001b[38;5;241m=\u001b[39m SarsaAgent(num_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, num_actions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)  \u001b[38;5;66;03m# Assuming 10x10 grid world\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Evaluate the trained agent\u001b[39;00m\n\u001b[1;32m     24\u001b[0m total_rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[28], line 10\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(env, agent, num_episodes)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m      9\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 10\u001b[0m     next_action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     agent\u001b[38;5;241m.\u001b[39mlearn(state, action, reward, next_state, next_action)\n\u001b[1;32m     12\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n",
      "Cell \u001b[0;32mIn[27], line 35\u001b[0m, in \u001b[0;36mSarsaAgent.choose_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_actions)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 11\u001b[0m, in \u001b[0;36mQTable.get_best_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_best_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 1 with size 4"
     ]
    }
   ],
   "source": [
    "def train_agent(env, agent, num_episodes=100):\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        state = env.reset()\n",
    "        action = agent.choose_action(state)\n",
    "        agent.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_action = agent.choose_action(next_state)\n",
    "            agent.learn(state, action, reward, next_state, next_action)\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "\n",
    "\n",
    "# Define environment and agent\n",
    "env = GridWorld()\n",
    "agent = SarsaAgent(num_states=20, num_actions=4)  # Assuming 10x10 grid world\n",
    "\n",
    "# Train the agent\n",
    "train_agent(env, agent)\n",
    "\n",
    "# Evaluate the trained agent\n",
    "total_rewards = 0\n",
    "num_episodes = 10\n",
    "for _ in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_rewards += reward\n",
    "print(\"Average reward per episode after training:\", total_rewards / num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
